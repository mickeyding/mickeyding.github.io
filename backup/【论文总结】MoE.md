# 论文list
1. ** DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models** 2024.01 

# DeepSeekMoE

![Image](https://github.com/user-attachments/assets/7d46c4f0-f661-4b05-82cd-15a0e44d1117)

**，它提出了一种名为 **DeepSeekMoE** 的新的Mixture-of-Experts（MoE）架构，旨在通过更好的专家专门化来提高MoE语言模型的性能。

## 1. **目的**：
文章的主要目的是解决当前MoE架构中的专家专门化问题。传统的MoE架构（如GShard）面临以下两个主要问题：
- **知识混杂**：每个专家被训练以处理不同类型的知识，这会导致专家在学习多种知识时效果不佳，尤其是当每个专家的知识过于广泛时。
- **知识冗余**：多个专家可能会学习相同或相似的知识，导致模型的参数冗余，并且不能有效地利用每个专家的独特性。

**DeepSeekMoE** 通过两种主要策略来应对这些问题：
- **精细的专家分割**：将专家的中间隐藏维度分割成更小的单位，从而提高激活的专家数量，使得每个专家可以专注于更细致的知识领域。
- **共享专家隔离**：一些专家被隔离并始终激活，目的是捕获并集中共通的知识，减少其他专家之间的冗余。

## 2. **具体方法**：

![Image](https://github.com/user-attachments/assets/4d80eef4-bea5-47a0-a28d-0d5f9410fe67)

### 2.1 **精细专家分割**（Fine-Grained Expert Segmentation）：

在传统的Mixture-of-Experts（MoE）架构中，每个专家的前馈网络（Feed-Forward Network，FFN）通常具有相同的隐藏维度。为了解决知识混杂的问题，DeepSeekMoE采用了精细的专家分割策略：

- **核心思想**：每个专家的FFN被分割成更小的“子专家”——通过减小FFN的隐藏维度，使得每个专家在学习过程中更加专注于某一类知识。这样，每个“子专家”专注于更狭窄的领域，从而避免了传统MoE架构中一个专家需要学习多个领域知识的情况。
  
- **具体实施**：
  - 假设一个标准专家FFN的隐藏维度为\(d\)，在DeepSeekMoE中，隐藏维度会被减少到原来的\( \frac{1}{m} \)，其中 \(m\) 是细分的倍数。
  - 这意味着，原本一个专家对应一个FFN网络，现在这个专家将被分割成\(m\)个较小的专家，保持总体计算量不变。
  - 为了弥补分割后带来的计算量下降，DeepSeekMoE通过增加激活的专家数量（即每个输入token会激活更多的专家）来维持相同的计算开销。

- **优势**：
  - 这种精细化的分割提高了专家间的专门化，每个小专家专注于处理更狭窄的知识类型。
  - 通过激活更多的细粒度专家，模型能够更加灵活地组合激活的专家，避免了专家之间的知识交叉和混杂。

### 2.2 **共享专家隔离**（Shared Expert Isolation）：

为了进一步减少冗余知识和确保不同专家间的专门化，DeepSeekMoE提出了“共享专家”策略。

- **核心思想**：共享专家是一些专家，它们的任务是捕获并共享通用的知识，这些共享专家在每次计算中都会被激活。而其他的专家（通常称为“路由专家”）则负责更具专门化的任务。
  
- **具体实施**：
  - 在每一层中，DeepSeekMoE会选择\( K_s \)个专家作为共享专家，这些专家会在每次计算中都被激活，旨在捕捉任务间共享的知识。
  - 由于共享专家捕捉的是通用知识，所以它们的参数在不同的token计算过程中都是通用的，这有效减少了知识冗余。
  - 与此相对的，剩下的专家则是路由专家，只会根据token的具体输入被激活。

- **优势**：
  - 共享专家有助于减少冗余，确保不同专家关注于特定的、独特的知识领域。
  - 共享专家的使用也有效提升了参数效率，避免了路由专家之间知识的重复存储。

![Image](https://github.com/user-attachments/assets/a78bdd03-0aa2-4dad-84a4-f60b859669e2)

$e^l_i$表示的是每个专家的质心，即可学习的embedding，用来计算和Token的相似度来投票出TopK个专家；

### 2.3 **专家级平衡损失（Expert-Level Balance Loss）**：

- **目标**：解决“路由崩溃”（routing collapse）的问题，确保每个专家都能得到足够的训练，而不是集中在少数几个专家上。它确保每个专家的使用情况是均衡的，而不是某些专家被频繁选中，其他专家被忽略。

- **核心思想**：通过引入平衡损失，模型鼓励尽可能公平地使用所有专家。这防止了专家的训练过于集中，导致某些专家过拟合，而其他专家未被充分训练。

- **具体实施**：
  - 计算每个专家的使用频率（即有多少token被分配给该专家）。
  - 使用一个平衡因子（\(\alpha_1\)）来控制这个损失项;
  - 损失函数通过计算所有专家的使用频率（\(f_i\)）和亲和力（\(P_i\)）的加权和来度量平衡性。具体的计算公式为：
   
![Image](https://github.com/user-attachments/assets/2c14f741-7d97-4501-90bf-541291498f95)

  - 其中，\( f_i \) 是专家\( i \)的使用频率，\( P_i \) 是该专家的亲和力。  T是专家要处理的全部的token数；N‘表示去掉共享专家后的路由专家的数量； K’表示激活路由专家的数量；I是指示函数
  - 公式13为什么要乘以N‘ 并除以K’ ？为了保持计算损失的恒定，不随专家数量的变化而变化

- **优势**：
  - 该损失确保了每个专家都能得到充分的训练，防止了少数几个专家占用模型大部分计算资源，保证了专家之间的训练更加均衡。

### 2.4 **设备级平衡损失（Device-Level Balance Loss）**：

- **目标**：解决计算瓶颈的问题，特别是当模型在多台设备上进行分布式训练时，确保各个设备的计算负载尽量平衡，从而避免某些设备的计算负担过重。

- **核心思想**：在多设备训练中，可能会出现某些设备计算负担过重，而其他设备负担较轻。设备级平衡损失的引入旨在确保负载平衡，使得每个设备的计算量大致相等，从而避免了计算瓶颈。

- **具体实施**：
  - 计算每个设备的计算负载（即分配给该设备的专家数量）。
  - 使用设备级平衡因子（\(\alpha_2\)）来控制损失项，确保负载平衡。损失函数的计算公式为：
  
![Image](https://github.com/user-attachments/assets/1b211089-1bb3-4700-b0cf-24862ea0d5b4)

  - 其中，\( f'_i \) 是分配给设备\( i \)的专家的平均使用频率，\( P'_i \) 是该设备的专家亲和力。
  
- **优势**：
  - 通过平衡每个设备的计算负载，减少了计算瓶颈，使得训练更加高效，能够在更大规模的分布式环境下有效地进行。


### 2.5 **参数和计算效率**：

DeepSeekMoE保持了传统MoE架构的计算效率，同时通过以上的策略进一步优化了计算量和参数的使用：

- **计算开销**：虽然DeepSeekMoE使用更多的激活专家，但通过精细的专家分割和共享专家的引入，计算量保持在合理范围内。
- **参数利用**：通过将专家分割成更小的部分并引入共享专家，DeepSeekMoE能更高效地使用参数，避免冗余和浪费。


### 3. **下游表现**：
DeepSeekMoE的下游表现显示了它在多个任务上的优越性，尤其是在以下几个方面：
- **语言建模**：通过在Pile数据集上的测试，DeepSeekMoE显示出比传统的MoE架构（如GShard）更好的性能，尤其在使用相同数量的计算和参数时。
- **推理与语言理解**：在多个基准任务（如HellaSwag、PIQA、RACE等）上，DeepSeekMoE在许多任务上超越了现有的MoE架构，显示出更好的推理和理解能力。
- **代码生成**：DeepSeekMoE在代码生成任务（如HumanEval和MBPP）上也表现突出，显示出更强的生成能力。
- **知识问答**：在诸如TriviaQA和NaturalQuestions这样的闭卷问答任务上，DeepSeekMoE表现出了较好的效果。

# DeepSeekV3




## MLA

