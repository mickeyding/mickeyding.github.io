# Paper List

1.Driving Scene Synthesis on Free-form Trajectories with Generative Prior [2024.12.02]
2.InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models [2024.12]
3.GenEx: Generating an Explorable World [2024.12]

# Driving Scene Synthesis on Free-form Trajectories with Generative Prior
![image](https://github.com/user-attachments/assets/3524244e-b78b-45cc-8ad6-1be14e74a944)

该方法利用视频生成模型当成先验，来更好的训练 3DGS 新视角生成的能力；

## 关键方法具体实现

### 1. 生成先验与逆问题设计

#### 目标
利用视频扩散模型作为生成先验，通过解决逆问题优化参数化的3D模型（如 Gaussian Splatting），生成高保真场景。

![image](https://github.com/user-attachments/assets/a64e8f80-678f-4d00-bb81-2f4a8a3d0c00)

#### 实现步骤
1. **生成渲染图像**：
   - 使用当前迭代的3D模型生成新轨迹的渲染图像 $ V'_t $。

2. **构建逆问题**：
   - 定义逆问题从带伪影的渲染图像 $ V'_t $恢复高质量图像 $V $：
     $$
     V'_t = f(V) + \epsilon
     $$
   - 使用视频扩散模型 $ D $ 解决问题，生成优化图像：
     $$
     V'_t, refine = D(V'_t, M)
     $$

3. **掩码生成**：
   - 比较渲染图像 $V'_t $与从记录轨迹生成的伪真实图像 $ \hat{V} $，通过 SSIM 生成不可靠区域掩码：
     $$
     M = 1(SSIM(V'_t, \hat{V}) < \tau)
     $$

4. **生成优化图像**：
   - 使用扩散模型在掩码的指导下优化伪影区域，同时保留可靠区域的细节。

---

### 2. 迭代优化

#### 目标
通过不断优化参数化的3D模型，逐步提升渲染质量。

#### 实现步骤
1. **初始优化**：
   - 使用常规重建方法对记录轨迹进行优化，获得初始高斯模型。

2. **生成新轨迹图像**：
   - 利用当前高斯模型渲染新轨迹视图的图像  $ V'_t $ ，并通过扩散模型生成优化版本  $ V'_t, refine $  。

3. **计算损失**：
   - 在记录轨迹上使用图像重建损失：
   
     $$
     L_{\text{img}} = \lambda * || I' - I ||_1 + (1 - \lambda) * L_{\text{SSIM}}
     $$

   - 在新轨迹上引入生成先验的损失：
     $$
     L' = L_{\text{img}}(V'_t, V'_t, refine)
     $$

4. **参数更新**：
   - 根据损失函数优化高斯模型参数 $ G $，并迭代提升模型性能。

5. **缓冲区机制**：
   - 每隔一定步数（如 2000 步）更新生成图像，平衡训练效率与质量。

## 任务表现
在 Waymo Open Dataset 中，通过新轨迹（偏移±1m、±2m、±3m）的图像评估，DriveX在FID（Fréchet Inception Distance）、道路车道IoU（Intersection over Union）以及车辆平均精度（AP）方面表现出提升

## 可能存在的问题
推测该方法依赖原始数据的训练出的初始3DGS，因为要依赖初始3DGS模型新视角的深度进行warp，计算unreliablility mask；如果这个mask计算的不准，会影响video diffusion的渲染结果；进而影响3DGS的优化；所以原始数据还是需要有一定的稠密度；

---------

# InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models
![image](https://github.com/user-attachments/assets/7f0380d0-4e0c-4728-9ab7-3992dafd5e58)

## 目标
InfiniCube旨在生成可控、高保真和大规模的动态3D驾驶场景，以支持自动驾驶车辆的训练和测试，以及混合现实和机器人技术的应用。具体目标包括：
1. **场景一致性与真实性**：保证几何与外观在动态场景中的一致性，支持可靠的物理仿真。
2. **大规模场景生成**：扩展到数万平方米的驾驶场景，超越以往方法的规模限制。
3. **灵活的控制能力**：通过输入的高精度(HD)地图、车辆边界框和文本描述来操控场景布局、外观以及车辆行为。

## **方法**
![image](https://github.com/user-attachments/assets/c59f2736-737d-42b1-bdf5-68370dc1172d)

InfiniCube采用了一个三阶段的方法来实现上述目标：
1. **无边界的体素世界生成**：
   - 利用基于稀疏体素的扩散模型，从输入的HD地图和车辆边界框生成一个大规模语义体素世界。基于SCube和XCube系列工作；输入分别是HDmap，路面，bbox的占用信息，输出是基于输入条件的3D voxel；
   
![image](https://github.com/user-attachments/assets/1f0be872-fa1f-4994-a1e5-eb1ca75b2c8c)

   - 通过“Outpainting”技术逐块扩展体素场景，保证不同块之间的几何一致性。

2. **基于世界的动态视频生成**：
   - 利用Stable Video Diffusion (SVD)视频生成模型，结合从体素世界渲染的“指导缓冲区”（包括语义缓冲区和坐标缓冲区），坐标缓冲区指的是pixel交汇的第一个voxel的3D位置，生成长时间、物理一致的动态视频。
   - 引入ControlNet生成视频的初始帧（SVD架构的限制），并允许通过文本描述调整场景外观（如天气条件）。

3. **动态3D场景生成**：
   - 提出了一种“双分支重建”方法：
     - **体素分支**：3D空间3DGS表示；从静态背景生成高质量的3DGS,主要是针对静态场景；
     - **像素分支**：FeedForward 3DGS；利用**视频帧中的深度信息**，**原始图像信息**，**Depth Anything V2的特征**，提升中景（除近景和天空部分）和动态物体的细节表现。
   - 最终合成为动态的3D Gaussian Splatting (3DGS) 场景，提供丰富的外观细节和全局可控性。

## 性能表现 
1. **长视频生成质量**：
   - **FID（Frechet Inception Distance）**：在长时间帧序列中（200帧），InfiniCube比对比方法（Panacea和Vista）保持更低的FID，视频质量和一致性显著更优。
   - **人类评价**：在HD地图对齐性测试中，InfiniCube的生成结果在40、80和120帧的对齐性上分别达到84.6%、83.9%、84.8%，显著优于Panacea。

2. **新视图合成性能**：
   - 使用PSNR、SSIM和LPIPS指标衡量新视图生成的质量：
     - InfiniCube在所有指标上优于SCube、PixelNeRF等基线方法，例如在T+5帧上的PSNR达20.80，SSIM达0.73，均为最高。

3. **场景细节生成与控制能力**：
   - 可通过简单修改文本描述生成不同天气条件（如晴天、雾天、雪天）或插入新车辆，且生成结果在视觉一致性和动态控制上表现出色。

## 总结
InfiniCube通过结合稀疏体素生成、视频生成和动态3D场景重建的方法，成功实现了高保真、可控且大规模的动态3D驾驶场景生成。整个方案涉及了Voxel Generation，Image Generation，Video Generation，3DGS reconstruction， FeedForward 3DGS Reconstruction，**对在工业界是否能直接应用（系统误差耦合误差的影响）有一些担忧**；重建和生成也没有真正融合，视频生成在这里还只是提供Texture相关的信息；大扰动下，3DGS的渲染质量和视频生成的质量应该还是会有差距，并没有解决这个问题；


-------------


# GenEx: Generating an Explorable World
![image](https://github.com/user-attachments/assets/538dbfea-165c-4412-96b7-163f814c915a)

## 目标
GenEx的主要目标是通过单张RGB图片生成可探索的3D一致虚拟环境，以支持人工智能（AI）在虚拟世界中的学习、导航和决策。具体目标包括：
1. **生成虚拟3D世界**：基于单张图像创建一个360°全景的虚拟世界，允许无限探索。
2. **增强AI能力**：通过“想象增强策略”（Imagination-Augmented Policy），帮助AI模拟环境，做出更可靠的决策。
3. **支持多场景应用**：在导航、虚拟现实（VR）、互动游戏和自主机器人领域，推动多智能体的协作与决策。

## 方法
GenEx结合了生成模型和智能体策略，提出了以下关键阶段：

### 1.世界生成（World Initialization）
- **目标**：基于单张RGB图像生成360°的全景虚拟世界，为AI提供沉浸式环境。
- **实现方法**：
  - **训练数据**：利用Unreal Engine和Unity中的物理引擎数据，生成高质量的虚拟环境，包括立方体投影（cubemap）和等距柱状投影（equirectangular panorama）等多种全景表示形式。
  - **模型架构**：
    - 采用FLUX.1扩散模型，将文本描述和单张图像作为条件输入，生成高动态范围的360°全景图像。
    - 改进的模型条件化机制确保生成的全景图像与输入图像一致。
  - **输出**：生成静态的360°全景虚拟环境，作为后续动态探索的起点。

### 2. 世界转换（World Transition）
- **目标**：随着智能体在虚拟环境中的移动，动态更新其视角，生成连续的全景视频。
- **实现方法**：
  - **动作驱动的视图更新**：
    - 智能体的动作由旋转角度 $\alpha_t$和移动距离 $d_t$）定义。
    - 给定当前的全景视图和智能体动作，使用旋转变换（sphere rotation）调整视角，以模拟球面运动。
  - **视频生成**：
    - 在调整后的视图基础上，利用扩展的全景视频扩散模型，生成新的前向视角视频。
    - 该模型在最新探索视图和随机噪声的条件下生成下一帧，确保视觉一致性和场景动态变化。
  - **球面一致性学习（Spherical-Consistency Learning, SCL）**：
    - 为了避免全景边缘不连续性，训练中引入SCL，使得全景图像在球面上保持平滑和连续。

### 3. 世界探索（World Exploration）
- **目标**：允许智能体根据不同的策略探索生成的虚拟世界。
- **探索模式**：
  - **交互式探索（Interactive Exploration）**：
    - 用户直接控制智能体的方向和移动，实时生成新的视图。
  - **GPT辅助探索（GPT-Assisted Exploration）**：
    - 使用GPT模型规划智能体动作，避免生成视频质量下降。
  - **目标驱动探索（Goal-Driven Navigation）**：
    - 智能体根据特定目标（如导航到某一物体）进行规划，并生成对应路径和环境视图。

### 4. 想象增强策略（Imagination-Augmented Policy）
- **目标**：结合生成环境的观察信息，帮助智能体模拟并优化决策。
- **步骤**：
  1. 智能体通过虚拟环境获取未观察到的部分信息，生成新的观察。
  2. 基于想象的观察信息，选择能够最大化决策策略的动作。
  3. 在多智能体场景中，预测其他智能体的观察和行为，从而优化自身决策。

## 性能表现
1. **生成质量**：
   - 采用视频生成指标（如FVD、SSIM、LPIPS、PSNR）评估，GenEx在所有指标上均优于基线模型，展现高保真和一致性（如PSNR达到30.2，SSIM为0.94）。
   - 支持生成高质量的鸟瞰图（Bird’s-eye View）和多视角3D场景。

2. **探索一致性**：
   - 通过“探索循环一致性”（IELC）测量长距离路径的鲁棒性，保持低漂移率（均方误差MSE低于0.1）。

3. **决策增强**：
   - 在“想象增强策略”中，结合生成环境的GPT-4模型在复杂决策任务中的准确率达到85.22%，显著高于只依赖文本或静态图像的基线模型。
   - 在多智能体场景中，GenEx显著提升了人类和智能体的推理和协作能力，决策准确率高达94.87%。

4. **多视角生成**：
   - 在多视角合成任务中，与其他SOTA方法相比，GenEx能够更好地生成高质量的背景一致场景和动态对象。

## 总结
GenEx通过结合生成AI和物理一致性的虚拟世界建模，提供了一个可探索、动态交互的生成环境，推动了嵌入式AI在导航、协作和决策中的应用。这种框架为跨领域应用（如机器人技术、VR/AR）奠定了基础，同时展示了生成模型在多智能体协作和复杂任务中的潜力。**引入了全景视频的生成保持loop consistency，全景视频的覆盖范围大**，所以对于持久性来说，视频生成模型的输入前一帧的条件 $x_{t-1}^s$包含的范围更大，但对于更大的范围，是否还能保证loop consistency？ 原理上至少没有保证；主要还是靠合成的数据的强3D一致性来保证生成效果，真实场景很难收集这样的数据来应用。