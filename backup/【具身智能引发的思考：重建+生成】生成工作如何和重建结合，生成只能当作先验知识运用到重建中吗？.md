# Paper List

1.Driving Scene Synthesis on Free-form Trajectories with Generative Prior [2024.12.02]
2.InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models [2024.12]

# Driving Scene Synthesis on Free-form Trajectories with Generative Prior
![image](https://github.com/user-attachments/assets/3524244e-b78b-45cc-8ad6-1be14e74a944)

该方法利用视频生成模型当成先验，来更好的训练 3DGS 新视角生成的能力；

## 关键方法具体实现

### 1. 生成先验与逆问题设计

#### 目标
利用视频扩散模型作为生成先验，通过解决逆问题优化参数化的3D模型（如 Gaussian Splatting），生成高保真场景。

![image](https://github.com/user-attachments/assets/a64e8f80-678f-4d00-bb81-2f4a8a3d0c00)

#### 实现步骤
1. **生成渲染图像**：
   - 使用当前迭代的3D模型生成新轨迹的渲染图像 $ V'_t $。

2. **构建逆问题**：
   - 定义逆问题从带伪影的渲染图像 $ V'_t $恢复高质量图像 $V $：
     $$
     V'_t = f(V) + \epsilon
     $$
   - 使用视频扩散模型 $ D $ 解决问题，生成优化图像：
     $$
     V'_t, refine = D(V'_t, M)
     $$

3. **掩码生成**：
   - 比较渲染图像 $V'_t $与从记录轨迹生成的伪真实图像 $ \hat{V} $，通过 SSIM 生成不可靠区域掩码：
     $$
     M = 1(SSIM(V'_t, \hat{V}) < \tau)
     $$

4. **生成优化图像**：
   - 使用扩散模型在掩码的指导下优化伪影区域，同时保留可靠区域的细节。

---

### 2. 迭代优化

#### 目标
通过不断优化参数化的3D模型，逐步提升渲染质量。

#### 实现步骤
1. **初始优化**：
   - 使用常规重建方法对记录轨迹进行优化，获得初始高斯模型。

2. **生成新轨迹图像**：
   - 利用当前高斯模型渲染新轨迹视图的图像  $ V'_t $ ，并通过扩散模型生成优化版本  $ V'_t, refine $  。

3. **计算损失**：
   - 在记录轨迹上使用图像重建损失：
   
     $$
     L_{\text{img}} = \lambda * || I' - I ||_1 + (1 - \lambda) * L_{\text{SSIM}}
     $$

   - 在新轨迹上引入生成先验的损失：
     $$
     L' = L_{\text{img}}(V'_t, V'_t, refine)
     $$

4. **参数更新**：
   - 根据损失函数优化高斯模型参数 $ G $，并迭代提升模型性能。

5. **缓冲区机制**：
   - 每隔一定步数（如 2000 步）更新生成图像，平衡训练效率与质量。

## 任务表现
在 Waymo Open Dataset 中，通过新轨迹（偏移±1m、±2m、±3m）的图像评估，DriveX在FID（Fréchet Inception Distance）、道路车道IoU（Intersection over Union）以及车辆平均精度（AP）方面表现出提升

## 可能存在的问题
推测该方法依赖原始数据的训练出的初始3DGS，因为要依赖初始3DGS模型新视角的深度进行warp，计算unreliablility mask；如果这个mask计算的不准，会影响video diffusion的渲染结果；进而影响3DGS的优化；所以原始数据还是需要有一定的稠密度；

---------

# InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models
![image](https://github.com/user-attachments/assets/7f0380d0-4e0c-4728-9ab7-3992dafd5e58)

## 目标
InfiniCube旨在生成可控、高保真和大规模的动态3D驾驶场景，以支持自动驾驶车辆的训练和测试，以及混合现实和机器人技术的应用。具体目标包括：
1. **场景一致性与真实性**：保证几何与外观在动态场景中的一致性，支持可靠的物理仿真。
2. **大规模场景生成**：扩展到数万平方米的驾驶场景，超越以往方法的规模限制。
3. **灵活的控制能力**：通过输入的高精度(HD)地图、车辆边界框和文本描述来操控场景布局、外观以及车辆行为。

## **方法**
![image](https://github.com/user-attachments/assets/c59f2736-737d-42b1-bdf5-68370dc1172d)

InfiniCube采用了一个三阶段的方法来实现上述目标：
1. **无边界的体素世界生成**：
   - 利用基于稀疏体素的扩散模型，从输入的HD地图和车辆边界框生成一个大规模语义体素世界。基于SCube和XCube系列工作；输入分别是HDmap，路面，bbox的占用信息，输出是基于输入条件的3D voxel；
   
![image](https://github.com/user-attachments/assets/1f0be872-fa1f-4994-a1e5-eb1ca75b2c8c)

   - 通过“Outpainting”技术逐块扩展体素场景，保证不同块之间的几何一致性。

2. **基于世界的动态视频生成**：
   - 利用Stable Video Diffusion (SVD)视频生成模型，结合从体素世界渲染的“指导缓冲区”（包括语义缓冲区和坐标缓冲区），坐标缓冲区指的是pixel交汇的第一个voxel的3D位置，生成长时间、物理一致的动态视频。
   - 引入ControlNet生成视频的初始帧（SVD架构的限制），并允许通过文本描述调整场景外观（如天气条件）。

3. **动态3D场景生成**：
   - 提出了一种“双分支重建”方法：
     - **体素分支**：3D空间3DGS表示；从静态背景生成高质量的3DGS,主要是针对静态场景；
     - **像素分支**：FeedForward 3DGS；利用**视频帧中的深度信息**，**原始图像信息**，**Depth Anything V2的特征**，提升中景（除近景和天空部分）和动态物体的细节表现。
   - 最终合成为动态的3D Gaussian Splatting (3DGS) 场景，提供丰富的外观细节和全局可控性。

## 性能表现 
1. **长视频生成质量**：
   - **FID（Frechet Inception Distance）**：在长时间帧序列中（200帧），InfiniCube比对比方法（Panacea和Vista）保持更低的FID，视频质量和一致性显著更优。
   - **人类评价**：在HD地图对齐性测试中，InfiniCube的生成结果在40、80和120帧的对齐性上分别达到84.6%、83.9%、84.8%，显著优于Panacea。

2. **新视图合成性能**：
   - 使用PSNR、SSIM和LPIPS指标衡量新视图生成的质量：
     - InfiniCube在所有指标上优于SCube、PixelNeRF等基线方法，例如在T+5帧上的PSNR达20.80，SSIM达0.73，均为最高。

3. **场景细节生成与控制能力**：
   - 可通过简单修改文本描述生成不同天气条件（如晴天、雾天、雪天）或插入新车辆，且生成结果在视觉一致性和动态控制上表现出色。

## 总结
InfiniCube通过结合稀疏体素生成、视频生成和动态3D场景重建的方法，成功实现了高保真、可控且大规模的动态3D驾驶场景生成。整个方案涉及了Voxel Generation，Image Generation，Video Generation，3DGS reconstruction， FeedForward 3DGS Reconstruction，**对其中的耦合和系统在工业界的应用误差有一些担忧**；重建和生成也没有真正融合，视频生成在这里还只是提供Texture相关的信息；大扰动下，3DGS的渲染质量和视频生成的质量应该还是会有差距，并没有解决这个问题；