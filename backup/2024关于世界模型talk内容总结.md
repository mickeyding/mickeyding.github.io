
# NIO IN 2024 NIO Innovation Technology Day

##  大脑的两个核心的能力：
- 空间理解：想象重建
- 时间理解：想象推演
以上两个能力即是时空的认知能力 -> 世界模型

## 蔚来世界模型NWM
从提取信息的角度，把世界模型和感知算法的迭代路径统一了
![image](https://github.com/user-attachments/assets/56fdfc7e-9772-4bea-b1e4-fe08a4b4aef8)

世界模型算法相比感知的优势：
![image](https://github.com/user-attachments/assets/1a6338fe-3a38-4c0d-be67-d321701f7267)

能想象变化才是真实的时空理解，想象的真实度和丰富度是理解深度的体现；

- 重建的真实性：来源于千万clips的学习
- 开发了新的时空encoding的方式
-  加入了智驾的指令

## 世界模型的穿越
让世界模型回到撞车前的3s中，世界模型可以推演出更合理的驾驶行为

## 轨迹规划
每0.1s生成216个平行的可能轨迹，评选后选出最优解

**猜测**：核心在于这句话；从这句话出发，蔚来的未披露的NWM的技术细节应该是依赖LLM类自回归的架构，类似GAIA-1；视频生成和轨迹生成应该是两个head；基于历史信息，预测未来帧的轨迹，没有提到Lidar，应该不是基于BEVFormer类似的多模态世界模型，应该就是自回归架构，类似ADrive-I同时输出图像和轨迹的token，其中轨迹的token可以当成文本的token处理，是一个多模态的自回归架构；

### 如果这个0.1s包括216种不同未来帧的视频生成的时间
- 1.蔚来的方案基于diffusion的video decoder，但花了很多工夫优化diffusion head的速度；基于diffusion的head 即使SD3的图像模型TensorRT加速后的速度也差不多在1s/帧（1080p）；7月份的模型，速度做到0.1s，并行度拉满到216， 也得在视频diffusion的模型上相比SD3加速10倍。
- 2.蔚来的方案只用vae下的video decoder的模型，该模型也可以做到时序上的平滑生成，效率比diffusion高效

个人倾向于0.1s不包括视频生成的时间；如果该速度没有考虑并行度，batchsize为1，一次infer的时间是0.00046s，按照gaia-1的分辨率，288*512， 一帧的token数量是18*32 = 576个，一个token的预测时间是8*10^-7s;  再考虑后续接diffusion head的时间，应该是远远不够，这部分我再仔细思考一下；

## NWM的优势
![image](https://github.com/user-attachments/assets/8a5307a6-f46f-4d8b-8de3-cf326c2f9310)

