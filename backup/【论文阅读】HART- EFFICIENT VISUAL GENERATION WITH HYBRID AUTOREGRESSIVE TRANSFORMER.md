
# 目的
本文提出了一种新型视觉生成模型——混合自回归变压器（HART）。其核心目标是实现高效、高分辨率的图像生成（1024×1024像素），并在图像质量上达到与扩散模型相当的水平，同时显著提升生成效率。为了解决现有自回归模型中离散编码器重建性能不足及训练成本高昂的问题，HART引入了混合编码器和高效的残差扩散模块。

#方法

本文的核心方法围绕混合编码器、残差扩散模块和可扩展分辨率自回归变压器设计，以下是具体实现细节：

## 一 **混合编码器**
混合编码器是 HART 的关键创新之一，通过结合离散和连续特征，提升了图像生成的质量和效率。
![image](https://github.com/user-attachments/assets/38fc07b3-d046-474c-9dc8-7edf3083d1c5)


1. **特征分解**：
   - 输入图像经过 CNN 编码器生成连续视觉特征（latent tokens）。
   - 使用向量量化（VQ）方法将连续特征量化为离散标记（discrete tokens），捕获图像整体结构信息。
   - 剩余部分（未能被离散标记捕获的细节信息）被视为残差，称为 **残差标记（residual tokens）**。

2. **交替训练**：
   - 训练过程中，每次迭代随机选择两种路径之一：
     - **离散路径**：模型仅利用离散标记进行图像重建，类似传统离散编码器。
     - **连续路径**：模型使用完整的连续特征进行图像重建，类似连续自编码器。
   - 这种交替训练确保模型能够充分利用离散和连续特征，并保持它们之间的协同性。

3. **训练与推理分离**：
   - 训练时，模型学习如何解码离散和连续标记。
   - 推理时，模型仅解码连续特征（即离散标记与残差标记的和），简化计算流程。


## 二 **残差扩散模块**
为高效建模连续标记的细节信息，HART 使用了一种轻量化的残差扩散模块。
![image](https://github.com/user-attachments/assets/f097d539-fa68-4985-bdfd-5dfa8b64e739)

###  **模块设计**：
   - **输入**：自回归变压器的最后一层隐层状态（the last layer hidden states from our scalable-resolution AR transformer）-> 近似模拟混合编码器中的连续视觉特征；
和离散标记（discrete tokens predicted in the last VAR sampling step）-> 不同分辨率下离散token进行上采样到最后一个分辨率再加和；

#### **自回归变压器的最后一层隐层状态（Hidden States）**
- **作用**：
  - 提供全局上下文信息，编码了离散标记和输入条件（如文本提示）之间的关系。
  - 类似连续特征的高维表征，保留了图像生成中的动态特性。
  
#### **离散标记（Discrete Tokens）**
- **来源**：
  - 从混合编码器的多尺度离散标记中获取。
  - 经过上采样，将所有尺度的离散标记统一到最终分辨率，并对各尺度标记进行求和。
  
- **作用**：
  - 提供图像的静态全局结构信息，作为辅助输入。

**文章没有明确说清楚是对什么加噪，既然模型的输出是residual tokens，那应该是将Encoder出来的连续特征 - 不同分辨率下离散token进行上采样到最后一个分辨率再加和得到的residual tokens进行加噪；然后condition为自回归变压器的最后一层隐层状态和离散标记；**

   - **模型结构**：基于多层感知机（MLP），显著简化了扩散模型的计算开销。
   
   - **输出**：和MAR不同，MAR predicts full continuous tokens, HART models residual tokens，因此训练难度降低，推理效率提升，仅需 8 步即可完成残差标记的去噪；





###  **采样过程**：
   - 训练阶段：采用标准扩散方法，通过 1000 步噪声日程对残差标记建模。
   - 推理阶段：仅需 8 步即可完成残差标记的去噪，相比传统扩散模型（通常需要 30-50 步），效率提升显著。

#### **Residual Tokens 的生成目标**
- **定义**：
  - 连续特征中未被离散标记捕获的细节部分，即：
    $$
    Residual Tokens =Encoder的连续特征 - 离散标记的总和
    $$
  - 这些细节信息表示图像中的高频成分，如细腻的纹理和细节。

####  **加噪机制**

**加噪对象**：  
加噪的应该是 **Residual Tokens**，即上述连续特征与离散标记的总和之间的差异。  

**过程理解**：
1. **加噪起点**：  

   $$
   \text{Residual Tokens}_{t=0} = \text{Encoder连续特征} - \text{Sum of Discrete Tokens}
   $$

   在扩散训练中，对 Residual Tokens 加入逐步递增的高斯噪声：

   $$
   \text{Residual Tokens}_t = \alpha_t \cdot \text{Residual Tokens}_{t=0} + \beta_t \cdot \mathcal{N}(0, I)
   $$

   其中$ \( t \) $ 为时间步，$\(\alpha_t\) $ 和 $ \(\beta_t\) $ 为加权系数。

3. **条件输入**：
   - 隐层状态和离散标记作为条件输入，帮助模型在每个时间步恢复高频信息。

4. **采样过程**：
   - 在推理阶段，通过扩散过程逐步去噪生成高质量的 Residual Tokens：
   
     $$
     \text{Residual Tokens}_{t-1} \to \text{Residual Tokens}_0
     $$

### **条件的作用**

#### **隐层状态的作用**
- 提供上下文信息：
  - 自回归变压器的隐层状态包含了图像的全局上下文关系，尤其是离散标记与文本提示之间的关联。
  - 对残差生成过程起到指导作用，确保高频细节与全局结构一致。

#### **离散标记的作用**
- 提供全局参考：
  - 离散标记总和提供了基础图像结构，约束 Residual Tokens 的生成方向。
  - 避免 Residual Tokens 模型重复生成全局结构，提升效率。


## 三 **可扩展分辨率自回归变压器**
HART 使用了一个专门设计的自回归变压器，用于高分辨率图像生成。

1. **多模态建模**：
   - 文本标记与图像标记在输入时被拼接在一起，使文本信息能够直接指导图像生成。
   - 文本标记使用 1D 相对位置编码，图像标记使用 2D 相对位置编码，保证在高分辨率条件下的灵活性。

2. **训练策略**：
   - 首先在低分辨率（256×256）数据集上预训练。
   - 然后使用相对位置编码插值技术，将低分辨率模型扩展到高分辨率（512×512 和 1024×1024），加速高分辨率训练收敛。

3. **生成过程**：
   - 通过直接生成 1024×1024 分辨率图像，避免传统超分辨率模型的中间步骤，进一步提升效率。

5. **效率优化**：
   - 为减少高分辨率训练成本，HART 在训练中丢弃 80% 的标记，仅对剩余标记应用监督。
   - 注意力机制中引入稀疏计算，使训练和推理成本显著降低。


## 四. **整体生成流程**
1. **输入处理**：
   - 文本提示被编码为文本标记，与离散标记拼接。
   - 初始生成离散标记，描述图像的大体结构。
2. **细节补充**：
   - 残差扩散模块生成残差标记，对细节进行补充。
3. **解码**：
   - 最终图像由离散标记和残差标记的和进行解码，得到高质量的输出。

