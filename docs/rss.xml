<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>NiaDing's Technical Blog </title><link>https://mickeyding.github.io</link><description>I have research and development experience in autonomous driving simulation, and have explored related technologies such as NeRF, 3DGS and diffusion. Here, I will be sharing my insights and knowledge with everyone.</description><copyright>NiaDing's Technical Blog </copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://mickeyding.github.io</link></image><lastBuildDate>Tue, 07 Jan 2025 07:48:12 +0000</lastBuildDate><managingEditor>NiaDing's Technical Blog </managingEditor><ttl>60</ttl><webMaster>NiaDing's Technical Blog </webMaster><item><title>【论文阅读】1x-worldmodel</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%911x-worldmodel.html</link><description>&#13;
# World Model在机器人领域的必要性&#13;
&#13;
1. 解决机器人评估中的挑战&#13;
&#13;
性能退化：机器人在多任务训练后，可能因环境背景或光照的细微变化，导致性能在短时间内迅速下降。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%911x-worldmodel.html</guid><pubDate>Thu, 02 Jan 2025 09:16:34 +0000</pubDate></item><item><title>【论文阅读】π0: A Vision-Language-Action Flow Model for General Robot Control</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91%CF%800-%20A%20Vision-Language-Action%20Flow%20Model%20for%20General%20Robot%20Control.html</link><description>![image](https://github.com/user-attachments/assets/6178940c-d7a8-42d9-861a-20d411875b77)&#13;
&#13;
&#13;
# 目标&#13;
这篇文章介绍了名为 **π0** 的模型，旨在解决机器人学习中的主要挑战，包括数据稀缺性、泛化能力和鲁棒性。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91%CF%800-%20A%20Vision-Language-Action%20Flow%20Model%20for%20General%20Robot%20Control.html</guid><pubDate>Thu, 02 Jan 2025 08:07:30 +0000</pubDate></item><item><title>【论文阅读】Wonderland: Navigating 3D Scenes from a Single Image</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Wonderland-%20Navigating%203D%20Scenes%20from%20a%20Single%20Image.html</link><description>&#13;
![image](https://github.com/user-attachments/assets/be7fb372-01ec-453f-9289-ebd2b91270af)&#13;
&#13;
# 目标&#13;
&#13;
这篇文章《Wonderland: Navigating 3D Scenes from a Single Image》提出了一种**基于单张图片的高效三维场景重建方法**，结合视频扩散模型和高斯点渲染技术，能够在无需逐场景优化的情况下快速生成高质量、宽范围的三维场景。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Wonderland-%20Navigating%203D%20Scenes%20from%20a%20Single%20Image.html</guid><pubDate>Thu, 19 Dec 2024 08:49:37 +0000</pubDate></item><item><title>【论文阅读】InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91InternLM-XComposer2.5-OmniLive-%20A%20Comprehensive%20Multimodal%20System%20for%20Long-term%20Streaming%20Video%20and%20Audio%20Interactions.html</link><description>![image](https://github.com/user-attachments/assets/70e682fe-6d38-4178-b8d1-8617428b046c)&#13;
&#13;
# 目标&#13;
&#13;
该文章提出了 InternLM-XComposer2.5-OmniLive (IXC2.5-OL)，一个具备长时间多模态（音频和视频）实时交互能力的AI系统&#13;
![image](https://github.com/user-attachments/assets/cd4fc1cc-6091-4b80-b4de-1287825aa2af)&#13;
&#13;
# 方法&#13;
## **记忆模块利用机制**&#13;
为了利用多模态长记忆模块并解决大规模数据的问题，训练过程中引入了**三大核心任务**，针对视频和音频数据的记忆管理：&#13;
&#13;
![image](https://github.com/user-attachments/assets/91e9c192-70f9-44e5-9534-41f9940f6989)&#13;
&#13;
&#13;
&#13;
### **1. 视频片段压缩任务**&#13;
- **目标**：通过**压缩器**（Compressor）模块，将视频短片段的信息提取并压缩为**短期记忆**和**全局记忆**。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91InternLM-XComposer2.5-OmniLive-%20A%20Comprehensive%20Multimodal%20System%20for%20Long-term%20Streaming%20Video%20and%20Audio%20Interactions.html</guid><pubDate>Tue, 17 Dec 2024 09:38:38 +0000</pubDate></item><item><title>矩阵基本变换</title><link>https://mickeyding.github.io/post/ju-zhen-ji-ben-bian-huan.html</link><description>## 位姿变换&#13;
&#13;
#### 各坐标系变换总结 &#13;
&#13;
![图片1](https://github.com/user-attachments/assets/5c33040e-56fe-4de2-9bfb-befb3bcb2971)&#13;
&#13;
&#13;
[//]: # 'Rx:右手系中绕x轴旋转矩阵；Ry:右手系中绕y轴 旋转矩阵；Rz:右手系中绕z轴旋转矩阵；'&#13;
&#13;
#### 旋转矩阵的变换&#13;
&#13;
##### 坐标系运动方向正负判定&#13;
&#13;
- 坐标系运动方向判断：右手使用右手螺旋定则，左手使用左手螺旋定则，大拇指方向为坐标轴指向，四指指向为正&#13;
&#13;
- 坐标系运动方向的正负和坐标点运动的方向相反&#13;
&#13;
  &#13;
&#13;
##### 变换顺序&#13;
&#13;
在右手坐标系中，是左结合，即最后一个矩阵最先应用&#13;
&#13;
##### 静态坐标轴旋转和动态坐标轴旋转&#13;
&#13;
如何从坐标系变换得到在初始坐标系下变换后的点的坐标？&#13;
&#13;
坐标系绕动态轴运算：**每一次是按变换后的新的坐标系的轴运算**，得到的角度取负再带入R矩阵求变换矩阵&#13;
&#13;
与坐标按初始坐标系固定轴旋转效果一致。</description><guid isPermaLink="true">https://mickeyding.github.io/post/ju-zhen-ji-ben-bian-huan.html</guid><pubDate>Tue, 17 Dec 2024 06:25:56 +0000</pubDate></item><item><title>【论文阅读】HUMAN-LIKE EPISODIC MEMORY FOR INFINITE CONTEXT LLMS</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91HUMAN-LIKE%20EPISODIC%20MEMORY%20FOR%20INFINITE%20CONTEXT%20LLMS.html</link><description>![image](https://github.com/user-attachments/assets/041fe49e-1c94-416b-8099-194793722b15)&#13;
&#13;
&#13;
# 目标&#13;
该文章的目的是引入EM-LLM框架，通过模拟人类的情景记忆机制，解决大语言模型（LLMs）在处理长上下文时的局限性​；EM-LLM的设计灵感源于人类大脑的事件认知与回忆过程，目标是使LLMs能够有效地处理无限长度的上下文，提高检索和推理性能，同时保持计算效率​；&#13;
&#13;
# 方法&#13;
&#13;
EM-LLM 的实现方法包括 **事件分段** 和 **记忆检索** 两个核心阶段，同时引入了基于**贝叶斯突发**的初始边界识别与**图论边界细化**方法，详细步骤如下：&#13;
&#13;
![image](https://github.com/user-attachments/assets/7c146151-c2ab-4e83-adbe-e2ef543c8656)&#13;
&#13;
---&#13;
&#13;
## **1. 事件分段（Memory Formation）**&#13;
&#13;
### **步骤 1：基于“贝叶斯突发”的初始事件边界识别**&#13;
- **目的**：通过**惊讶值（Surprise）** 识别事件边界。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91HUMAN-LIKE%20EPISODIC%20MEMORY%20FOR%20INFINITE%20CONTEXT%20LLMS.html</guid><pubDate>Tue, 17 Dec 2024 03:52:19 +0000</pubDate></item><item><title>【论文阅读】HART: EFFICIENT VISUAL GENERATION WITH HYBRID AUTOREGRESSIVE TRANSFORMER</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91HART-%20EFFICIENT%20VISUAL%20GENERATION%20WITH%20HYBRID%20AUTOREGRESSIVE%20TRANSFORMER.html</link><description># 目的&#13;
本文提出了一种新型视觉生成模型——混合自回归变压器（HART）。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91HART-%20EFFICIENT%20VISUAL%20GENERATION%20WITH%20HYBRID%20AUTOREGRESSIVE%20TRANSFORMER.html</guid><pubDate>Thu, 12 Dec 2024 12:13:03 +0000</pubDate></item><item><title>【论文阅读--端到端自动驾驶】DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du----duan-dao-duan-zi-dong-jia-shi-%E3%80%91DiffusionDrive-%20Truncated%20Diffusion%20Model%20for%20End-to-End%20Autonomous%20Driving.html</link><description># 目标&#13;
DiffusionDrive旨在解决端到端自动驾驶中的实时性、多模式决策和复杂场景适应问题，通过高效生成多种驾驶轨迹，实现精准、鲁棒的驾驶规划。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du----duan-dao-duan-zi-dong-jia-shi-%E3%80%91DiffusionDrive-%20Truncated%20Diffusion%20Model%20for%20End-to-End%20Autonomous%20Driving.html</guid><pubDate>Wed, 04 Dec 2024 12:45:15 +0000</pubDate></item><item><title>【具身智能引发的思考：重建+生成】生成工作如何和重建结合，生成只能当作先验知识运用到重建中吗？</title><link>https://mickeyding.github.io/post/%E3%80%90-ju-shen-zhi-neng-yin-fa-de-si-kao-%EF%BC%9A-zhong-jian-%2B-sheng-cheng-%E3%80%91-sheng-cheng-gong-zuo-ru-he-he-zhong-jian-jie-he-%EF%BC%8C-sheng-cheng-zhi-neng-dang-zuo-xian-yan-zhi-shi-yun-yong-dao-zhong-jian-zhong-ma-%EF%BC%9F.html</link><description># Paper List&#13;
&#13;
1.Driving Scene Synthesis on Free-form Trajectories with Generative Prior [2024.12.02]&#13;
2.InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models [2024.12]&#13;
3.GenEx: Generating an Explorable World [2024.12]&#13;
&#13;
# Driving Scene Synthesis on Free-form Trajectories with Generative Prior&#13;
![image](https://github.com/user-attachments/assets/3524244e-b78b-45cc-8ad6-1be14e74a944)&#13;
&#13;
该方法利用视频生成模型当成先验，来更好的训练 3DGS 新视角生成的能力；&#13;
&#13;
## 关键方法具体实现&#13;
&#13;
### 1. 生成先验与逆问题设计&#13;
&#13;
#### 目标&#13;
利用视频扩散模型作为生成先验，通过解决逆问题优化参数化的3D模型（如 Gaussian Splatting），生成高保真场景。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-ju-shen-zhi-neng-yin-fa-de-si-kao-%EF%BC%9A-zhong-jian-%2B-sheng-cheng-%E3%80%91-sheng-cheng-gong-zuo-ru-he-he-zhong-jian-jie-he-%EF%BC%8C-sheng-cheng-zhi-neng-dang-zuo-xian-yan-zhi-shi-yun-yong-dao-zhong-jian-zhong-ma-%EF%BC%9F.html</guid><pubDate>Wed, 04 Dec 2024 09:18:51 +0000</pubDate></item><item><title>【论文阅读】JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91JanusFlow-%20Harmonizing%20Autoregression%20and%20Rectified%20Flow%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html</link><description># **JanusFlow 实现详解**&#13;
&#13;
JanusFlow 是一个结合 **整流流（Rectified Flow）** 和 **任务解耦策略** 的多模态统一框架，以下是其实现的关键内容。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91JanusFlow-%20Harmonizing%20Autoregression%20and%20Rectified%20Flow%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html</guid><pubDate>Thu, 28 Nov 2024 02:24:12 +0000</pubDate></item><item><title>2024关于世界模型talk内容总结</title><link>https://mickeyding.github.io/post/2024-guan-yu-shi-jie-mo-xing-talk-nei-rong-zong-jie.html</link><description># NIO IN 2024 NIO Innovation Technology Day&#13;
&#13;
##  大脑的两个核心的能力：&#13;
- 空间理解：想象重建&#13;
- 时间理解：想象推演&#13;
以上两个能力即是时空的认知能力 -&gt; 世界模型&#13;
&#13;
## 蔚来世界模型NWM&#13;
从提取信息的角度，把世界模型和感知算法的迭代路径统一了&#13;
![image](https://github.com/user-attachments/assets/56fdfc7e-9772-4bea-b1e4-fe08a4b4aef8)&#13;
&#13;
世界模型算法相比感知的优势：&#13;
![image](https://github.com/user-attachments/assets/1a6338fe-3a38-4c0d-be67-d321701f7267)&#13;
&#13;
能想象变化才是真实的时空理解，想象的真实度和丰富度是理解深度的体现；&#13;
&#13;
”大脑就像一个“动态模拟器”，它能够在当前的时空框架中，模拟出多个不同的变化情景。</description><guid isPermaLink="true">https://mickeyding.github.io/post/2024-guan-yu-shi-jie-mo-xing-talk-nei-rong-zong-jie.html</guid><pubDate>Tue, 19 Nov 2024 10:07:18 +0000</pubDate></item><item><title>【论文整理】Instance-level Control for Image Generation</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-zheng-li-%E3%80%91Instance-level%20Control%20for%20Image%20Generation.html</link><description># PaperList&#13;
- IFAdapter: Instance feature control for grounded Text-to-Image Generation（2024.09）&#13;
- MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis（2024.07）&#13;
- InstanceDiffusion:Instance-level Control for Image Generation（2024.02）&#13;
- MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis （2024.02）&#13;
- HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation （2024.10）&#13;
&#13;
。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-zheng-li-%E3%80%91Instance-level%20Control%20for%20Image%20Generation.html</guid><pubDate>Thu, 24 Oct 2024 02:13:10 +0000</pubDate></item><item><title>【论文阅读】Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Janus-%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html</link><description># 总结&#13;
Janus通过解耦视觉编码来解决理解和生成任务对视觉编码器不同需求之间的冲突，从而提升模型在这两个领域的表现；采用一个自回归框架，没有用到diffusion来做图像生成，是完全的自回归文本预测和图像生成任务；&#13;
- 优势：论文给出的例子在文本理解和文本控制生成上确实是有提升；离不开鲁棒扎实的数据工程的处理，训练范式的探索工作；在GenEval基准测试中，超过了DALL-E 2和SDXL等文本到图像的生成模型；&#13;
- 劣势：离散特征下的自回归图像生成任务是大家做多模态渐渐抛弃的思路，比如show-o抛弃了自回归做图像生成，transfusion抛弃了离散的图像特征，论文中也指出，下一步是利用diffusion技术来进一步提升表现；三阶段的训练过程 + 繁琐的数据工程给复现带来了难度。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Janus-%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html</guid><pubDate>Wed, 23 Oct 2024 03:07:06 +0000</pubDate></item><item><title>【论文阅读】MONOFORMER: ONE TRANSFORMER FOR BOTH DIFFUSION AND AUTOREGRESSION</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91MONOFORMER-%20ONE%20TRANSFORMER%20FOR%20BOTH%20DIFFUSION%20AND%20AUTOREGRESSION.html</link><description>MonoFormer 是一种多模态生成模型，基本想法和Transfusion类似，它通过共享一个Transformer模型实现了文本自回归和图像扩散两种生成任务。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91MONOFORMER-%20ONE%20TRANSFORMER%20FOR%20BOTH%20DIFFUSION%20AND%20AUTOREGRESSION.html</guid><pubDate>Wed, 16 Oct 2024 02:14:23 +0000</pubDate></item><item><title>【论文阅读】Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Transfusion-%20Predict%20the%20Next%20Token%20and%20Diffuse%20Images%20with%20One%20Multi-Modal%20Model.html</link><description>该模型能够在离散数据（如文本）和连续数据（如图像）上进行训练，并集成了语言模型的“下一个词预测”任务和扩散模型的图像生成能力；Transfusion 基本模型在多个基准任务上表现一般，最高配置模型在图像生成方面，其生成质量与其他扩散模型如SDXL等相当，但模型参数量是其2+倍；&#13;
&#13;
## 具体方案&#13;
&#13;
- 联合多模态损失函数：Transfusion模型将语言建模（next-token prediction）与图像扩散损失相结合，模型在训练过程中同时暴露于这两种不同的模态和相应的损失函数。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Transfusion-%20Predict%20the%20Next%20Token%20and%20Diffuse%20Images%20with%20One%20Multi-Modal%20Model.html</guid><pubDate>Thu, 19 Sep 2024 08:26:27 +0000</pubDate></item><item><title>【论文阅读】Show-o: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Show-o-%20ONE%20SINGLE%20TRANSFORMER%20TO%20UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION.html</link><description>SHOW-O 通过一个单一的Transformer架构，引入**离散去噪过程**处理图像的生成任务，LLM任务采用因果attention，图像生成任务采用全局attention，统一了多模态理解和生成任务，无需多个专门的模型。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Show-o-%20ONE%20SINGLE%20TRANSFORMER%20TO%20UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION.html</guid><pubDate>Wed, 18 Sep 2024 04:27:35 +0000</pubDate></item><item><title>StableDiffusion3代码分析</title><link>https://mickeyding.github.io/post/StableDiffusion3-dai-ma-fen-xi.html</link><description># StableDiffusion3Pipeline&#13;
## 步骤和算法&#13;
**输入检查：**&#13;
调用 [check_inputs]方法，验证输入参数的有效性。</description><guid isPermaLink="true">https://mickeyding.github.io/post/StableDiffusion3-dai-ma-fen-xi.html</guid><pubDate>Sat, 14 Sep 2024 02:17:54 +0000</pubDate></item></channel></rss>