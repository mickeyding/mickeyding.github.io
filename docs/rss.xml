<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>NiaDing's Technical Blog </title><link>https://mickeyding.github.io</link><description>I have research and development experience in autonomous driving simulation, and have explored related technologies such as NeRF, 3DGS and diffusion. Here, I will be sharing my insights and knowledge with everyone.</description><copyright>NiaDing's Technical Blog </copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://mickeyding.github.io</link></image><lastBuildDate>Wed, 20 Nov 2024 03:47:59 +0000</lastBuildDate><managingEditor>NiaDing's Technical Blog </managingEditor><ttl>60</ttl><webMaster>NiaDing's Technical Blog </webMaster><item><title>2024关于世界模型talk内容总结</title><link>https://mickeyding.github.io/post/2024-guan-yu-shi-jie-mo-xing-talk-nei-rong-zong-jie.html</link><description># NIO IN 2024 NIO Innovation Technology Day&#13;
&#13;
##  大脑的两个核心的能力：&#13;
- 空间理解：想象重建&#13;
- 时间理解：想象推演&#13;
以上两个能力即是时空的认知能力 -&gt; 世界模型&#13;
&#13;
## 蔚来世界模型NWM&#13;
从提取信息的角度，把世界模型和感知算法的迭代路径统一了&#13;
![image](https://github.com/user-attachments/assets/56fdfc7e-9772-4bea-b1e4-fe08a4b4aef8)&#13;
&#13;
世界模型算法相比感知的优势：&#13;
![image](https://github.com/user-attachments/assets/1a6338fe-3a38-4c0d-be67-d321701f7267)&#13;
&#13;
能想象变化才是真实的时空理解，想象的真实度和丰富度是理解深度的体现；&#13;
&#13;
”大脑就像一个“动态模拟器”，它能够在当前的时空框架中，模拟出多个不同的变化情景。</description><guid isPermaLink="true">https://mickeyding.github.io/post/2024-guan-yu-shi-jie-mo-xing-talk-nei-rong-zong-jie.html</guid><pubDate>Tue, 19 Nov 2024 10:07:18 +0000</pubDate></item><item><title>【论文整理】Instance-level Control for Image Generation</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-zheng-li-%E3%80%91Instance-level%20Control%20for%20Image%20Generation.html</link><description># PaperList&#13;
- IFAdapter: Instance feature control for grounded Text-to-Image Generation（2024.09）&#13;
- MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis（2024.07）&#13;
- InstanceDiffusion:Instance-level Control for Image Generation（2024.02）&#13;
- MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis （2024.02）&#13;
- HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation （2024.10）&#13;
&#13;
。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-zheng-li-%E3%80%91Instance-level%20Control%20for%20Image%20Generation.html</guid><pubDate>Thu, 24 Oct 2024 02:13:10 +0000</pubDate></item><item><title>【论文阅读】Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Janus-%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html</link><description># 总结&#13;
Janus通过解耦视觉编码来解决理解和生成任务对视觉编码器不同需求之间的冲突，从而提升模型在这两个领域的表现；采用一个自回归框架，没有用到diffusion来做图像生成，是完全的自回归文本预测和图像生成任务；&#13;
- 优势：论文给出的例子在文本理解和文本控制生成上确实是有提升；离不开鲁棒扎实的数据工程的处理，训练范式的探索工作；在GenEval基准测试中，超过了DALL-E 2和SDXL等文本到图像的生成模型；&#13;
- 劣势：离散特征下的自回归图像生成任务是大家做多模态渐渐抛弃的思路，比如show-o抛弃了自回归做图像生成，transfusion抛弃了离散的图像特征，论文中也指出，下一步是利用diffusion技术来进一步提升表现；三阶段的训练过程 + 繁琐的数据工程给复现带来了难度。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Janus-%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html</guid><pubDate>Wed, 23 Oct 2024 03:07:06 +0000</pubDate></item><item><title>【论文阅读】MONOFORMER: ONE TRANSFORMER FOR BOTH DIFFUSION AND AUTOREGRESSION</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91MONOFORMER-%20ONE%20TRANSFORMER%20FOR%20BOTH%20DIFFUSION%20AND%20AUTOREGRESSION.html</link><description>MonoFormer 是一种多模态生成模型，基本想法和Transfusion类似，它通过共享一个Transformer模型实现了文本自回归和图像扩散两种生成任务。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91MONOFORMER-%20ONE%20TRANSFORMER%20FOR%20BOTH%20DIFFUSION%20AND%20AUTOREGRESSION.html</guid><pubDate>Wed, 16 Oct 2024 02:14:23 +0000</pubDate></item><item><title>【论文阅读】Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Transfusion-%20Predict%20the%20Next%20Token%20and%20Diffuse%20Images%20with%20One%20Multi-Modal%20Model.html</link><description>该模型能够在离散数据（如文本）和连续数据（如图像）上进行训练，并集成了语言模型的“下一个词预测”任务和扩散模型的图像生成能力；Transfusion 基本模型在多个基准任务上表现一般，最高配置模型在图像生成方面，其生成质量与其他扩散模型如SDXL等相当，但模型参数量是其2+倍；&#13;
&#13;
## 具体方案&#13;
&#13;
- 联合多模态损失函数：Transfusion模型将语言建模（next-token prediction）与图像扩散损失相结合，模型在训练过程中同时暴露于这两种不同的模态和相应的损失函数。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Transfusion-%20Predict%20the%20Next%20Token%20and%20Diffuse%20Images%20with%20One%20Multi-Modal%20Model.html</guid><pubDate>Thu, 19 Sep 2024 08:26:27 +0000</pubDate></item><item><title>【论文阅读】Show-o: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION</title><link>https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Show-o-%20ONE%20SINGLE%20TRANSFORMER%20TO%20UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION.html</link><description>SHOW-O 通过一个单一的Transformer架构，引入**离散去噪过程**处理图像的生成任务，LLM任务采用因果attention，图像生成任务采用全局attention，统一了多模态理解和生成任务，无需多个专门的模型。</description><guid isPermaLink="true">https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Show-o-%20ONE%20SINGLE%20TRANSFORMER%20TO%20UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION.html</guid><pubDate>Wed, 18 Sep 2024 04:27:35 +0000</pubDate></item><item><title>StableDiffusion3代码分析</title><link>https://mickeyding.github.io/post/StableDiffusion3-dai-ma-fen-xi.html</link><description># StableDiffusion3Pipeline&#13;
## 步骤和算法&#13;
**输入检查：**&#13;
调用 [check_inputs]方法，验证输入参数的有效性。</description><guid isPermaLink="true">https://mickeyding.github.io/post/StableDiffusion3-dai-ma-fen-xi.html</guid><pubDate>Sat, 14 Sep 2024 02:17:54 +0000</pubDate></item></channel></rss>